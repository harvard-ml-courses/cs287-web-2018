tas:
  - name: "<a href='mailto:yoonkim@g.harvard.edu'>Yoon</a> (OH: Monday 7-8pm MD (2nd floor lounge)),"
  - name: "<a href='mailto:carldenton@college.harvard.edu'>Carl</a> (OH: Monday 8-9pm Lowell)"


dates: ["", "Jan. 23", "Jan. 25", "Jan. 30", "Feb. 1", "Feb. 6", "Feb. 8", "Feb. 13", "Feb. 15","Feb. 20", "Feb. 22", "Feb. 27.", "Mar. 1", "Mar. 6.", "Mar. 8", "Mar. 20", "Mar. 22", "Mar. 27", "Mar. 29", "Apr. 3", "Apr. 5", "Apr. 10", "Apr. 12", "Apr. 17", "Apr. 19", "Apr. 24", "
2nd", "May 9th"]
ohs:
  - time: Tuesday 2:30-4pm
    location: MD 217 (Sasha)



lectures:
  - topic: Sequence Classification
    subtopic: <a href="lecture1.pdf">Intro</a>| Bag-of-Words
    hw:
    papers:
      - name: "Deep Learning Tutorial"
        section: Torch
        link: "http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
      - name: "Baselines and Bigrams"
        section: Fundamental
        link: "http://www.aclweb.org/anthology/P/P12/P12-2.pdf#page=118"
      - name: "Word Representations in Vector Space"
        link: "http://arxiv.org/abs/1301.3781"
      - name: "Deep Learning, NLP, and Representations"
        section: Blog
        link: "http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"
    section:
    demos:
  - topic:
    subtopic: Convolutions
    hw:
    papers:
      - name: "NLP Tutorial (Basic)"
        section: Torch
        link: "http://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html"
      - name: "Natural language processing (almost) from scratch"
        section: Fundamental
        link: "https://arxiv.org/abs/1103.0398"
      - name: "CNNs for Sentence Classification"
        link: "http://aclweb.org/anthology/D/D14/D14-1181.pdf"
      - name: "Understanding Convolutional Neural Networks for NLP"
        section: Blog
        link: "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
    section:
    demos:
  - topic: Sequence Modeling
    hw:  "<a href='https://github.com/harvard-ml-courses/cs287-s18/blob/master/HW1/Homework%201.ipynb'>Classification</a> <a href='https://www.kaggle.com/c/harvard-cs281-hw1'>(Kaggle)</a>"
    subtopic: NNLMs
    papers:
      - name: "Learning Embeddings"
        link: "http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
        section: Torch
      - name: "A Neural Probabilistic Language Model"
        section: Fundamental
        link: "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
      - name: "Hierarchical Probabilistic Neural Network Language Model"
        link: "https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf"
  - topic:
    subtopic: RNNs
    hw:
    papers:
      - name: "NLP Tutorial RNNs"
        section: Torch
        link: "http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
      - name: "Recurrent Neural Network-Based Language Model"
        link: "http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf"
        section: Fundamental
      - name: "Generating Text with Recurrent Neural Networks"
        link: "http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf"
      - name: "Understanding LSTM Networks"
        section: Blog
        link: "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      - name: "The Unreasonable Effectiveness of Recurrent Neural Networks"
        link: "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
      - name: A Bit of Progress in Language Modeling
        link: https://arxiv.org/abs/cs/0108005
        section: Extra
      - name: Exploring the Limits of Language Modeling (Billion word)
        link: https://arxiv.org/pdf/1602.02410.pdf
      - name: Do Multi-Sense Embeddings Improve Natural Language Understanding?
        link: https://arxiv.org/abs/1506.01070
      - name: TensorBoardX 
        link: https://github.com/lanpa/tensorboard-pytorch
    section:
    demos:
  - topic: Sequence Transduction
    subtopic: Encoding
    papers:
      - name: "Sequence-to-Sequence Learning with Recurrent Neural Networks"
        section: Fundamental
        link: "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"
      - name: "Named Entity Recognition with Bidirectional LSTM-CNNs"
        section: Application
        link: "https://www.aclweb.org/anthology/Q16-1026"
      - name: "Deep contextualized word representations (new)"
        link: "https://openreview.net/pdf?id=S1p31z-Ab"
  - topic:
    subtopic: Attention
    papers:
      - name: "Translation with a Sequence to Sequence Network and Attention"
        section: Torch
        link: "http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      - name: "NMT by Jointly Learning to Align and Translate"
        section: Fundamental
        link: "https://arxiv.org/abs/1409.0473"
      - name: "Show, Attend, and Tell"
        link: "https://arxiv.org/pdf/1502.03044.pdf"
      - name: "Attention and Augmented Recurrent Neural Networks"
        section: Blog
        link: "https://distill.pub/2016/augmented-rnns/"
    section:
    demos:
  - topic:
    subtopic: Search
    hw: <a href="https://github.com/harvard-ml-courses/cs287-s18/blob/master/HW2/Homework%202.ipynb">Modeling</a> <a href="https://www.kaggle.com/c/cs287-hw2-s18">(Kaggle)</a>
    papers:
      - name: " Sequence-to-Sequence Learning as Beam-Search Optimization"
        section: Application
        link: "https://arxiv.org/pdf/1606.02960.pdf"
      - name: "Sequence-level Training with Recurrent Neural Networks"
        link: "https://arxiv.org/pdf/1511.06732.pdf"
  - topic: Variational Inference
    hw:
    subtopic: Basics
    papers:
      - name: "Variational Inference: A Review for Statisticians"
        section: Tutorial
        link: "https://arxiv.org/abs/1601.00670"

  - topic:
    subtopic: VAEs
    hw:
    papers:
      - section: Torch
        name: "Variational Autoencoders"
        link: "https://github.com/pytorch/examples/tree/master/vae"
      - section: Fundamental
        name: "Auto-Encoding Variable Bayes"
        link: "https://arxiv.org/abs/1312.6114"
      - name: "Semi-Supervised Learning with Deep Generative Models"
        link: "https://arxiv.org/abs/1406.5298"
    section:
    demos:
  - topic: Other Latent Variable Techniques
    subtopic: GANs
    papers:
      - name: "GANs in 50 lines of code"
        section: Torch
        link: "https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f"
        link: "http://pytorch.org/docs/master/distributions.html"

      - name: "Generative Adversarial Networks"
        link: "https://arxiv.org/abs/1406.2661"
        section: Fundamental
      - name: "NIPS 2016 GAN Tutorial"
        section: Tutorial
        link: "https://arxiv.org/pdf/1701.00160.pdf"

  - topic:
    hw: <a href="https://github.com/harvard-ml-courses/cs287-s18/HW3/">Translation</a> <a href="https://www.kaggle.com/c/cs287-hw3-s18">(Kaggle)</a>
    subtopic: REINFORCE
    papers:
      - name: Distributions and Sampling
        section: Torch
        link: "http://pytorch.org/docs/master/distributions.html"
      - name: SeqGAN
        link: "https://arxiv.org/abs/1609.05473"
        section: "Application"
      - name: "Gradient Estimation Using Stochastic Computation Graphs"
        section: Tutorial
        link: "https://arxiv.org/abs/1506.05254"
  - topic: Guest Lecture (Structured Training for NMT)
    subtopic:
    hw:
  - topic: Midterm
    subtopic:
    hw:
  - topic: Latent Variable Models
    subtopic: Sequence VAEs
    papers:
      - name: "Generating Sentences from a Continuous Space"
        link: "https://arxiv.org/pdf/1511.06349.pdf"
      - name: "A Recurrent Latent Variable Model for Sequential Data"
        link: "http://papers.nips.cc/paper/5653-a-recurrent-latent-variable-model-for-sequential-data.pdf"
      - name: "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions"
        link: "https://arxiv.org/pdf/1702.08139.pdf"
  - topic:
    hw: <a href="https://github.com/harvard-ml-courses/cs287-s18/HW4/">VI</a> <a href="https://www.kaggle.com/c/cs287-hw4-s18">(Kaggle)</a>
    subtopic: Sampling Improvements
    hw:
  - topic:
    subtopic: Structured VAEs
    papers:
      - name: "Structured Inference Networks for Nonlinear State Space Models"
        link: "https://arxiv.org/pdf/1609.09869.pdf"
      - name: "Composing graphical models with neural networks for structured representations and fast inference"
        link: "https://arxiv.org/pdf/1603.06277.pdf"
    hw: Final Project Topics
  - topic: Discrete Variables
    subtopic: REINFORCE for text
  - topic:
    subtopic: Discrete VAEs
    hw: Project Abstract Due
  - topic: Dynamic Networks
    subtopic: Training with Search
  - topic:
    subtopic: Neural Module Networks
  - topic: NMT
    subtopic: Recent advances in NMT
  - topic:
    subtopic: Unsupervised NMT
    hw:
  - topic: Topics
    subtopic: Latent Variable Models in Science
  - topic:
    subtopic:  Latent Variable Models in Vision
  - topic: Interpretation
    subtopic:  Rationale Generation
  - topic: Final Presentation Talks (Evening)
    subtopic:  
  - topic: Final Paper Due
    subtopic:  
